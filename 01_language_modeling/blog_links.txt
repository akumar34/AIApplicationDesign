1. basic seq2seq blog:
https://towardsdatascience.com/nlp-sequence-to-sequence-networks-part-2-seq2seq-model-encoderdecoder-model-6c22e29fd7e1

2. seq2seq + attention
https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3
https://medium.com/@jbetker/implementing-seq2seq-with-attention-in-keras-63565c8e498c
https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb
https://medium.com/@jbetker/implementing-seq2seq-with-attention-in-keras-63565c8e498c

3. Transformer
https://jalammar.github.io/illustrated-transformer/

3. BERT, Elmo, etc.
http://jalammar.github.io/illustrated-bert/

4. Attention 
https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html 
http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/ 
